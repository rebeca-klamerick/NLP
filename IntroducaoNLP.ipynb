{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLDU1+pydKMlzPCveU4KoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebeca-klamerick/NLP/blob/main/IntroducaoNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìù Notebook - Classifica√ß√£o de Spam com NLP usando TensorFlow**\n",
        "###**1. Introdu√ß√£o ao NLP (Natural Language Processing)**\n",
        "O Processamento de Linguagem Natural (NLP) √© uma √°rea da intelig√™ncia artificial (IA) que permite que os computadores compreendam, interpretem e gerenciem dados de linguagem humana. No contexto de classifica√ß√£o de texto, como detectar mensagens spam ou n√£o spam, o NLP desempenha um papel fundamental ao transformar o texto em uma forma que os modelos de aprendizado de m√°quina possam entender.\n",
        "\n",
        "No nosso caso, o objetivo √© construir um modelo simples de classifica√ß√£o bin√°ria, onde queremos identificar se uma mensagem √© spam (1) ou n√£o spam (0). Para isso, usaremos uma abordagem de aprendizado de m√°quina com redes neurais.\n",
        "\n",
        "### **2. Prepara√ß√£o dos Dados**\n",
        "Primeiro, vamos criar um conjunto de dados simples de exemplos de mensagens de texto e r√≥tulos indicando se elas s√£o spam ou n√£o.\n",
        "\n",
        "###**3. Bibliotecas Utilizadas**\n",
        "Para construir o nosso modelo de NLP, utilizaremos a poderosa biblioteca TensorFlow e suas sub-bibliotecas Keras para a cria√ß√£o e treinamento do modelo de rede neural. Al√©m disso, usaremos o Tokenizer do Keras para transformar as palavras em n√∫meros, um passo essencial no processamento de texto.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# # vamos utilizar esse pequeno texto como dado de exemplo\n",
        "texts = [\n",
        "    \"Ganho dinheiro r√°pido com este m√©todo!\",\n",
        "    \"Promo√ß√£o imperd√≠vel! Compre agora e economize.\",\n",
        "    \"Oi, tudo bem? Podemos marcar uma reuni√£o amanh√£?\",\n",
        "    \"Fique atento √†s novidades do nosso blog.\"\n",
        "]\n",
        "labels = [1, 1, 0, 0]  # 1 = spam, 0 = n√£o spam\n",
        "```\n",
        "texts: S√£o as mensagens de texto que queremos classificar.\n",
        "\n",
        "labels: S√£o os r√≥tulos associados a cada mensagem, onde 1 significa spam e 0 significa n√£o spam.\n",
        "\n"
      ],
      "metadata": {
        "id": "bclPu9KhBNbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AToLmeL5BT9k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, as bibliotecas s√£o usadas para:\n",
        "\n",
        "Tokeniza√ß√£o: Converter texto em n√∫meros que podem ser processados pela rede neural.\n",
        "\n",
        "Cria√ß√£o do modelo: Definir a rede neural com camadas como Embedding e GlobalAveragePooling.\n",
        "\n",
        "Treinamento e Avalia√ß√£o: Compilar e treinar o modelo."
      ],
      "metadata": {
        "id": "BKrFAjgUDZWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de exemplo\n",
        "texts = [\n",
        "    \"Ganho dinheiro r√°pido com este m√©todo!\",\n",
        "    \"Promo√ß√£o imperd√≠vel! Compre agora e economize.\",\n",
        "    \"Oi, tudo bem? Podemos marcar uma reuni√£o amanh√£?\",\n",
        "    \"Fique atento √†s novidades do nosso blog.\"\n",
        "]\n",
        "labels = [1, 1, 0, 0]  # 1 = spam, 0 = n√£o spam"
      ],
      "metadata": {
        "id": "SIKcudBREEh7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4. Pr√©-processamento dos Dados**\n",
        "A primeira etapa do nosso pipeline √© tokenizar os textos. A tokeniza√ß√£o √© o processo de converter palavras em n√∫meros inteiros. Em seguida, usaremos padding para garantir que todas as sequ√™ncias de texto tenham o mesmo comprimento."
      ],
      "metadata": {
        "id": "jsQlSz9sDq6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Tokeniza√ß√£o e convers√£o para n√∫meros\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding para padronizar o tamanho das sequ√™ncias\n",
        "X_padded = pad_sequences(X, maxlen=10, padding=\"post\")\n",
        "X_padded = np.array(X_padded, dtype=np.float32)  # Garante que seja um array NumPy com floats\n",
        "labels = np.array(labels, dtype=np.float32)  # Converte labels para float"
      ],
      "metadata": {
        "id": "MewrqCaED5EZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, estamos:\n",
        "\n",
        "1.   Usando o Tokenizer para mapear as palavras para n√∫meros inteiros.\n",
        "2.   Aplicando padding para que todas as sequ√™ncias de texto tenham o mesmo comprimento de 10 palavras.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_fpbZuWMEOTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5. Constru√ß√£o do Modelo**\n",
        "Agora que nossos dados est√£o prontos, podemos construir o modelo de rede neural. Utilizamos uma camada Embedding para transformar os n√∫meros em vetores de palavras. Em seguida, aplicamos a camada GlobalAveragePooling1D para condensar a informa√ß√£o da sequ√™ncia e, por fim, temos uma camada Dense com ativa√ß√£o sigmoid para gerar uma previs√£o bin√°ria (spam ou n√£o spam).\n"
      ],
      "metadata": {
        "id": "0levajrJEhEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando o modelo de rede neural\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=16, input_length=10),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFsZTnBhEwZ3",
        "outputId": "cd8e884f-70a1-494c-8d64-a857c772c6f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste c√≥digo estamos fazendo o seguinte:\n",
        "\n",
        "*   Embedding: Converte os n√∫meros das palavras em vetores de tamanho fixo.\n",
        "*   GlobalAveragePooling1D: Reduz a dimensionalidade das sequ√™ncias.\n",
        "*   Dense: Camada totalmente conectada para fazer as previs√µes (1 para spam, 0 para n√£o spam).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-F9UzBGBL2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6. Treinamento do Modelo**\n",
        "Agora vamos treinar o modelo usando nossos dados de entrada (X_padded) e os r√≥tulos (labels). O modelo ser√° treinado por 10 √©pocas, e a m√©trica de acur√°cia ser√° monitorada."
      ],
      "metadata": {
        "id": "cLzlzzQSF1LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento do modelo\n",
        "model.fit(X_padded, labels, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-9sVL0nGCLS",
        "outputId": "6a9471b5-30d7-4cc2-b61e-7e083e5ac9f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 0.6933\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5000 - loss: 0.6919\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.6905\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 0.6890\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.6875\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.6863\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.6850\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 1.0000 - loss: 0.6834\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6817\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6799\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78af84999a90>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7. Testando o Modelo**\n",
        "Ap√≥s o treinamento, vamos testar o modelo com novas mensagens para ver como ele se comporta em dados desconhecidos. Aqui est√£o algumas novas mensagens de exemplo:"
      ],
      "metadata": {
        "id": "aIbe9hfMGNcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de teste\n",
        "test_texts = [\"Compre agora e aproveite!\", \"Ol√°, gostaria de marcar um caf√©.\"]\n",
        "X_test = tokenizer.texts_to_sequences(test_texts)\n",
        "X_test_padded = pad_sequences(X_test, maxlen=10, padding=\"post\")\n",
        "X_test_padded = np.array(X_test_padded, dtype=np.float32)\n",
        "\n",
        "# Fazer previs√µes\n",
        "predictions = model.predict(X_test_padded)\n",
        "\n",
        "for text, pred in zip(test_texts, predictions):\n",
        "    print(f\"Mensagem: {text} ‚Üí Probabilidade de ser spam: {pred[0]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmr7kOznGJzz",
        "outputId": "5d4ba5f9-c6b0-4e44-f2c9-e81971cb0728"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "Mensagem: Compre agora e aproveite! ‚Üí Probabilidade de ser spam: 0.5114\n",
            "Mensagem: Ol√°, gostaria de marcar um caf√©. ‚Üí Probabilidade de ser spam: 0.5052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse c√≥digo gera a probabilidade de cada mensagem ser spam. Quanto mais pr√≥xima de 1, mais prov√°vel √© que a mensagem seja spam."
      ],
      "metadata": {
        "id": "V4JfEPIvGVJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Convertendo previs√µes para 0 ou 1 com limiar de 0.5\n",
        "predicted_labels = (predictions.flatten() > 0.5).astype(int)\n",
        "\n",
        "print(\"Acur√°cia:\", accuracy_score(true_labels, predicted_labels))\n",
        "print(\"Precis√£o:\", precision_score(true_labels, predicted_labels))\n",
        "print(\"Recall:\", recall_score(true_labels, predicted_labels))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m8wclOYGvRF",
        "outputId": "5cc999c5-26a6-4a93-ca21-655d00e1a292"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acur√°cia: 0.5\n",
            "Precis√£o: 0.5\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**8. An√°lise de Desempenho (Acur√°cia e Res√≠duos)**\n",
        "A acur√°cia de 50% pode indicar que o modelo n√£o aprendeu de forma eficaz, devido a dois fatores principais:\n",
        "\n",
        "* Modelo simples: O modelo utilizado √© bastante b√°sico, com apenas algumas camadas, o que pode ser insuficiente para capturar padr√µes complexos nos dados de texto.\n",
        "\n",
        "* Dados limitados: Com apenas 4 exemplos de treinamento, √© dif√≠cil para qualquer modelo generalizar e aprender boas representa√ß√µes. Modelos de Machine Learning geralmente requerem uma quantidade significativa de dados para alcan√ßar um bom desempenho.\n",
        "\n",
        "A precis√£o (como a acur√°cia) √© uma m√©trica que pode ser afetada pela desbalanceamento nos dados ou pela quantidade de dados insuficiente. A seguir, voc√™ pode calcular a precis√£o, o recall e outros indicadores de desempenho, al√©m de analisar os res√≠duos das previs√µes:"
      ],
      "metadata": {
        "id": "bgF_I9_DGkEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**9. Conclus√£o**\n",
        "Neste notebook, exploramos como construir um modelo simples de classifica√ß√£o de spam usando Processamento de Linguagem Natural (NLP) com TensorFlow, apenas um modelinho bem did√°tico e simples. Embora o modelo tenha uma acur√°cia de 50%, isso √© esperado devido ao tamanho muito pequeno do conjunto de dados e √† simplicidade da arquitetura utilizada. Para melhorar os resultados, seria necess√°rio usar mais dados de treinamento e possivelmente ajustar a arquitetura do modelo, como adicionar camadas LSTM ou GRU para capturar depend√™ncias temporais no texto.\n",
        "\n",
        "Esse exemplo ilustra o processo b√°sico de tokeniza√ß√£o, prepara√ß√£o de dados, constru√ß√£o de modelo, treinamento e avalia√ß√£o. Agora, voc√™ pode experimentar com mais dados, ajustar o modelo e testar novas t√©cnicas para obter melhores resultados."
      ],
      "metadata": {
        "id": "_bG6-iebG70n"
      }
    }
  ]
}