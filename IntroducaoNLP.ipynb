{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLDU1+pydKMlzPCveU4KoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebeca-klamerick/NLP/blob/main/IntroducaoNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ“ Notebook - ClassificaÃ§Ã£o de Spam com NLP usando TensorFlow**\n",
        "###**1. IntroduÃ§Ã£o ao NLP (Natural Language Processing)**\n",
        "O Processamento de Linguagem Natural (NLP) Ã© uma Ã¡rea da inteligÃªncia artificial (IA) que permite que os computadores compreendam, interpretem e gerenciem dados de linguagem humana. No contexto de classificaÃ§Ã£o de texto, como detectar mensagens spam ou nÃ£o spam, o NLP desempenha um papel fundamental ao transformar o texto em uma forma que os modelos de aprendizado de mÃ¡quina possam entender.\n",
        "\n",
        "No nosso caso, o objetivo Ã© construir um modelo simples de classificaÃ§Ã£o binÃ¡ria, onde queremos identificar se uma mensagem Ã© spam (1) ou nÃ£o spam (0). Para isso, usaremos uma abordagem de aprendizado de mÃ¡quina com redes neurais.\n",
        "\n",
        "### **2. PreparaÃ§Ã£o dos Dados**\n",
        "Primeiro, vamos criar um conjunto de dados simples de exemplos de mensagens de texto e rÃ³tulos indicando se elas sÃ£o spam ou nÃ£o.\n",
        "\n",
        "###**3. Bibliotecas Utilizadas**\n",
        "Para construir o nosso modelo de NLP, utilizaremos a poderosa biblioteca TensorFlow e suas sub-bibliotecas Keras para a criaÃ§Ã£o e treinamento do modelo de rede neural. AlÃ©m disso, usaremos o Tokenizer do Keras para transformar as palavras em nÃºmeros, um passo essencial no processamento de texto.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# # vamos utilizar esse pequeno texto como dado de exemplo\n",
        "texts = [\n",
        "    \"Ganho dinheiro rÃ¡pido com este mÃ©todo!\",\n",
        "    \"PromoÃ§Ã£o imperdÃ­vel! Compre agora e economize.\",\n",
        "    \"Oi, tudo bem? Podemos marcar uma reuniÃ£o amanhÃ£?\",\n",
        "    \"Fique atento Ã s novidades do nosso blog.\"\n",
        "]\n",
        "labels = [1, 1, 0, 0]  # 1 = spam, 0 = nÃ£o spam\n",
        "```\n",
        "texts: SÃ£o as mensagens de texto que queremos classificar.\n",
        "\n",
        "labels: SÃ£o os rÃ³tulos associados a cada mensagem, onde 1 significa spam e 0 significa nÃ£o spam.\n",
        "\n"
      ],
      "metadata": {
        "id": "bclPu9KhBNbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AToLmeL5BT9k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, as bibliotecas sÃ£o usadas para:\n",
        "\n",
        "TokenizaÃ§Ã£o: Converter texto em nÃºmeros que podem ser processados pela rede neural.\n",
        "\n",
        "CriaÃ§Ã£o do modelo: Definir a rede neural com camadas como Embedding e GlobalAveragePooling.\n",
        "\n",
        "Treinamento e AvaliaÃ§Ã£o: Compilar e treinar o modelo."
      ],
      "metadata": {
        "id": "BKrFAjgUDZWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de exemplo\n",
        "texts = [\n",
        "    \"Ganho dinheiro rÃ¡pido com este mÃ©todo!\",\n",
        "    \"PromoÃ§Ã£o imperdÃ­vel! Compre agora e economize.\",\n",
        "    \"Oi, tudo bem? Podemos marcar uma reuniÃ£o amanhÃ£?\",\n",
        "    \"Fique atento Ã s novidades do nosso blog.\"\n",
        "]\n",
        "labels = [1, 1, 0, 0]  # 1 = spam, 0 = nÃ£o spam"
      ],
      "metadata": {
        "id": "SIKcudBREEh7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4. PrÃ©-processamento dos Dados**\n",
        "A primeira etapa do nosso pipeline Ã© tokenizar os textos. A tokenizaÃ§Ã£o Ã© o processo de converter palavras em nÃºmeros inteiros. Em seguida, usaremos padding para garantir que todas as sequÃªncias de texto tenham o mesmo comprimento."
      ],
      "metadata": {
        "id": "jsQlSz9sDq6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #TokenizaÃ§Ã£o e conversÃ£o para nÃºmeros\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding para padronizar o tamanho das sequÃªncias\n",
        "X_padded = pad_sequences(X, maxlen=10, padding=\"post\")\n",
        "X_padded = np.array(X_padded, dtype=np.float32)  # Garante que seja um array NumPy com floats\n",
        "labels = np.array(labels, dtype=np.float32)  # Converte labels para float"
      ],
      "metadata": {
        "id": "MewrqCaED5EZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, estamos:\n",
        "\n",
        "1.   Usando o Tokenizer para mapear as palavras para nÃºmeros inteiros.\n",
        "2.   Aplicando padding para que todas as sequÃªncias de texto tenham o mesmo comprimento de 10 palavras.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_fpbZuWMEOTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5. ConstruÃ§Ã£o do Modelo**\n",
        "Agora que nossos dados estÃ£o prontos, podemos construir o modelo de rede neural. Utilizamos uma camada Embedding para transformar os nÃºmeros em vetores de palavras. Em seguida, aplicamos a camada GlobalAveragePooling1D para condensar a informaÃ§Ã£o da sequÃªncia e, por fim, temos uma camada Dense com ativaÃ§Ã£o sigmoid para gerar uma previsÃ£o binÃ¡ria (spam ou nÃ£o spam).\n"
      ],
      "metadata": {
        "id": "0levajrJEhEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando o modelo de rede neural\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=16, input_length=10),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFsZTnBhEwZ3",
        "outputId": "cd8e884f-70a1-494c-8d64-a857c772c6f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste cÃ³digo estamos fazendo o seguinte:\n",
        "\n",
        "*   Embedding: Converte os nÃºmeros das palavras em vetores de tamanho fixo.\n",
        "*   GlobalAveragePooling1D: Reduz a dimensionalidade das sequÃªncias.\n",
        "*   Dense: Camada totalmente conectada para fazer as previsÃµes (1 para spam, 0 para nÃ£o spam).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-F9UzBGBL2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6. Treinamento do Modelo**\n",
        "Agora vamos treinar o modelo usando nossos dados de entrada (X_padded) e os rÃ³tulos (labels). O modelo serÃ¡ treinado por 10 Ã©pocas, e a mÃ©trica de acurÃ¡cia serÃ¡ monitorada."
      ],
      "metadata": {
        "id": "cLzlzzQSF1LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento do modelo\n",
        "model.fit(X_padded, labels, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-9sVL0nGCLS",
        "outputId": "6a9471b5-30d7-4cc2-b61e-7e083e5ac9f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 0.6933\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5000 - loss: 0.6919\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.6905\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 0.6890\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.6875\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.6863\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.6850\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 1.0000 - loss: 0.6834\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6817\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6799\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78af84999a90>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7. Testando o Modelo**\n",
        "ApÃ³s o treinamento, vamos testar o modelo com novas mensagens para ver como ele se comporta em dados desconhecidos. Aqui estÃ£o algumas novas mensagens de exemplo:"
      ],
      "metadata": {
        "id": "aIbe9hfMGNcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de teste\n",
        "test_texts = [\"Compre agora e aproveite!\", \"OlÃ¡, gostaria de marcar um cafÃ©.\"]\n",
        "X_test = tokenizer.texts_to_sequences(test_texts)\n",
        "X_test_padded = pad_sequences(X_test, maxlen=10, padding=\"post\")\n",
        "X_test_padded = np.array(X_test_padded, dtype=np.float32)\n",
        "\n",
        "# Fazer previsÃµes\n",
        "predictions = model.predict(X_test_padded)\n",
        "\n",
        "for text, pred in zip(test_texts, predictions):\n",
        "    print(f\"Mensagem: {text} â†’ Probabilidade de ser spam: {pred[0]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmr7kOznGJzz",
        "outputId": "5d4ba5f9-c6b0-4e44-f2c9-e81971cb0728"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "Mensagem: Compre agora e aproveite! â†’ Probabilidade de ser spam: 0.5114\n",
            "Mensagem: OlÃ¡, gostaria de marcar um cafÃ©. â†’ Probabilidade de ser spam: 0.5052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse cÃ³digo gera a probabilidade de cada mensagem ser spam. Quanto mais prÃ³xima de 1, mais provÃ¡vel Ã© que a mensagem seja spam."
      ],
      "metadata": {
        "id": "V4JfEPIvGVJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Convertendo previsÃµes para 0 ou 1 com limiar de 0.5\n",
        "predicted_labels = (predictions.flatten() > 0.5).astype(int)\n",
        "\n",
        "print(\"AcurÃ¡cia:\", accuracy_score(true_labels, predicted_labels))\n",
        "print(\"PrecisÃ£o:\", precision_score(true_labels, predicted_labels))\n",
        "print(\"Recall:\", recall_score(true_labels, predicted_labels))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m8wclOYGvRF",
        "outputId": "5cc999c5-26a6-4a93-ca21-655d00e1a292"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AcurÃ¡cia: 0.5\n",
            "PrecisÃ£o: 0.5\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**8. AnÃ¡lise de Desempenho (AcurÃ¡cia e ResÃ­duos)**\n",
        "A acurÃ¡cia de 50% pode indicar que o modelo nÃ£o aprendeu de forma eficaz, devido a dois fatores principais:\n",
        "\n",
        "* Modelo simples: O modelo utilizado Ã© bastante bÃ¡sico, com apenas algumas camadas, o que pode ser insuficiente para capturar padrÃµes complexos nos dados de texto.\n",
        "\n",
        "* Dados limitados: Com apenas 4 exemplos de treinamento, Ã© difÃ­cil para qualquer modelo generalizar e aprender boas representaÃ§Ãµes. Modelos de Machine Learning geralmente requerem uma quantidade significativa de dados para alcanÃ§ar um bom desempenho.\n",
        "\n",
        "A precisÃ£o (como a acurÃ¡cia) Ã© uma mÃ©trica que pode ser afetada pela desbalanceamento nos dados ou pela quantidade de dados insuficiente. A seguir, vocÃª pode calcular a precisÃ£o, o recall e outros indicadores de desempenho, alÃ©m de analisar os resÃ­duos das previsÃµes:"
      ],
      "metadata": {
        "id": "bgF_I9_DGkEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**9. ConclusÃ£o**\n",
        "Neste notebook, exploramos como construir um modelo simples de classificaÃ§Ã£o de spam usando Processamento de Linguagem Natural (NLP) com TensorFlow, apenas um modelinho bem didÃ¡tico e simples. Embora o modelo tenha uma acurÃ¡cia de 50%, isso Ã© esperado devido ao tamanho muito pequeno do conjunto de dados e Ã  simplicidade da arquitetura utilizada. Para melhorar os resultados, seria necessÃ¡rio usar mais dados de treinamento e possivelmente ajustar a arquitetura do modelo, como adicionar camadas LSTM ou GRU para capturar dependÃªncias temporais no texto.\n",
        "\n",
        "Esse exemplo ilustra o processo bÃ¡sico de tokenizaÃ§Ã£o, preparaÃ§Ã£o de dados, construÃ§Ã£o de modelo, treinamento e avaliaÃ§Ã£o. Agora, vocÃª pode experimentar com mais dados, ajustar o modelo e testar novas tÃ©cnicas para obter melhores resultados."
      ],
      "metadata": {
        "id": "_bG6-iebG70n"
      }
    }
  ]
}